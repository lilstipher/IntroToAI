{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project P2: Auto Encoder\n",
    "---\n",
    "\n",
    "Summary: \n",
    "- Perform basic unsupervised learning tasks using sklearn\n",
    "- Apply unsupervised learning on PyRat datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Autoencoder ?\n",
    "--\n",
    "An autoencoder is an artificial neural network used for unsupervised learning of efficient codings. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data. Some of the most powerful AI in the 2010s involves stacking sparse autoencoders in a deep learning network.\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Autoencoder\">Source Wikipedia</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure \n",
    "-- \n",
    "<img src=\"structure.png\" alt=\"Structure\"/>\n",
    "<a href=\"https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\">Image Source</a>\n",
    "\n",
    "An autoencoder always consists of two parts, the encoder and the decoder, which can be defined as transitions ${\\displaystyle \\phi }$  and  ${\\displaystyle \\psi ,}$  such that:\n",
    "\n",
    "$${\\displaystyle \\phi :{\\mathcal {X}}\\rightarrow {\\mathcal {F}}} (encoder)$$\n",
    "$${\\displaystyle \\psi :{\\mathcal {F}}\\rightarrow {\\mathcal {X}}} (decoder)$$ \n",
    "$${\\displaystyle \\phi ,\\psi ={\\underset {\\phi ,\\psi }{\\operatorname {arg\\,min} }}\\,\\|X-(\\psi \\circ \\phi )X\\|^{2}}$$ \n",
    "In the simplest case, where there is one hidden layer, the encoder stage of an autoencoder takes the input ${\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{d}={\\mathcal {X}}}$  and maps it to ${\\displaystyle \\mathbf {z} \\in \\mathbb {R} ^{p}={\\mathcal {F}}}$ :\n",
    "\n",
    "${\\displaystyle \\mathbf {z} =\\sigma (\\mathbf {Wx} +\\mathbf {b} )} $\n",
    "\n",
    "This image ${\\displaystyle \\mathbf {z} }$  is usually referred to as code, latent variables, or latent representation. Here, ${\\displaystyle \\sigma }$  is an element-wise activation function such as a sigmoid function or a rectified linear unit. ${\\displaystyle \\mathbf {W} }$  is a weight matrix and ${\\displaystyle \\mathbf {b} }$  is a bias vector. After that, the decoder stage of the autoencoder maps ${\\displaystyle \\mathbf {z} }$  to the reconstruction ${\\displaystyle \\mathbf {x'} }$  of the same shape as ${\\displaystyle \\mathbf {x} }$ :\n",
    "\n",
    "$${\\displaystyle \\mathbf {x'} =\\sigma '(\\mathbf {W'z} +\\mathbf {b'} )} $$\n",
    "where ${\\displaystyle \\mathbf {\\sigma '} ,\\mathbf {W'} ,{\\text{ and }}\\mathbf {b'} }$  for the decoder may differ in general from the corresponding ${\\displaystyle \\mathbf {\\sigma } ,\\mathbf {W} ,{\\text{ and }}\\mathbf {b} }$  for the encoder, depending on the design of the autoencoder.\n",
    "\n",
    "Autoencoders are also trained to minimise reconstruction errors (such as squared errors):\n",
    "\n",
    "$${\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x'} )=\\|\\mathbf {x} -\\mathbf {x'} \\|^{2}=\\|\\mathbf {x} -\\sigma '(\\mathbf {W'} (\\sigma (\\mathbf {Wx} +\\mathbf {b} ))+\\mathbf {b'} )\\|^{2}} $$\n",
    "where ${\\displaystyle \\mathbf {x} }$  is usually averaged over some input training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application on MNIST dataset :)\n",
    "--\n",
    "We'll use keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f04e4f2d4262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.datasets import mnist\n",
    "from keras.regularizers import l1\n",
    "import tqdm\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "tqdm.tqdm(print(x_train.shape))\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-51bb2a1989d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 128\n",
    "code_size = 32\n",
    "\n",
    "input_img = Input(shape=(input_size,))\n",
    "hidden_1 = Dense(hidden_size, activation='relu')(input_img)\n",
    "code = Dense(code_size, activation='relu')(hidden_1)\n",
    "hidden_2 = Dense(hidden_size, activation='relu')(code)\n",
    "output_img = Dense(input_size, activation='sigmoid')(hidden_2)\n",
    "\n",
    "autoencoder = Model(input_img, output_img)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
